{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Assignment5_RL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "deepnote_notebook_id": "ead403eb-d822-40b6-bcc2-cb7a1e22c2b6",
    "deepnote_execution_queue": []
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH0so9wFsYrA"
      },
      "source": [
        "# DAT405 - Assignment 5\r\n",
        "### Martin Hansson 20 hours \r\n",
        "### Joakim Bake 20 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O73TxqjJH7e7",
        "cell_id": "00000-07e38a24-e656-4cf4-a4a0-4402e2b10b79",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "**DAT405 Introduction to Data Science and AI, 2010-2021, Study Period 2** <br/>\n",
        "**Assignment 5: Reinforcement learning and Classification** <br/>\n",
        "**Due Date: Dec 9, 23:59** <br/>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**What to submit**\n",
        "*   **The entire assignment should be submitted through the notebook. No separate file will be accepted.** You can submit either the notebook itself, or a public link to a Google Colab notebook<br/>\n",
        "\n",
        "*In the notebook:*\n",
        "*\tState your names and how many hours each person spent on the assignment.\n",
        "*\tThe solutions and answers to the theoretical and practical problems, including LaTeX math-mode equations, plots and tables etc.\n",
        "*\tAll plots/results should be visible such that the notebook does not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.<br/>\n",
        "\n",
        "*Before submitting:*\n",
        "*   Make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDuY3qwbH7e7",
        "cell_id": "00001-d0918a93-1f43-4c3f-984c-c4d1dc2fcc81",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "**Self-check**<br/>\n",
        "Is all the required information included? Have you answered all questions to the best of your ability? Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?) Does your notebook run and can reproduce the results, plots and tables?\n",
        "\n",
        "**Grading**<br/>\n",
        "Grading will be based on a qualitative assessment of each assignment. It is important to:\n",
        "*\tPresent clear arguments\n",
        "*\tPresent the results in a pedagogical way\n",
        "*\tShow understanding of the topics (e.g, write a pseudocode) \n",
        "*\tGive correct solutions\n",
        "*\tMake sure that the code is well commented \n",
        "\n",
        "**Again, as mentioned in general guidelines, all code should be written here. And this same ipython notebook file (Assignment5_Reinforcement_Learning.ipynb) should be submitted with answers and code written in it. No separate file will be accepted.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_6obY12H7e7",
        "cell_id": "00002-e8ccf0da-b7f8-40ba-8ad8-21b074611d0d",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Primer\n",
        "\n",
        "## Decision Making\n",
        "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n",
        "two parts. First, how do we learn about the world? This involves both the\n",
        "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n",
        "currently know about the world, how should we decide what to do, taking into\n",
        "account future events and observations that may change our conclusions?\n",
        "Typically, this will involve creating long-term plans covering possible future\n",
        "eventualities. That is, when planning under uncertainty, we also need to take\n",
        "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n",
        "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n",
        "known to produce good results and experiment with something new is known\n",
        "as the **exploration-exploitation dilemma**.\n",
        "\n",
        "## The exploration-exploitation trade-off\n",
        "\n",
        "Consider the problem of selecting a restaurant to go to during a vacation. Lets say the\n",
        "best restaurant you have found so far was **Les Epinards**. The food there is\n",
        "usually to your taste and satisfactory. However, a well-known recommendations\n",
        "website suggests that **King’s Arm** is really good! It is tempting to try it out. But\n",
        "there is a risk involved. It may turn out to be much worse than **Les Epinards**,\n",
        "in which case you will regret going there. On the other hand, it could also be\n",
        "much better. What should you do?\n",
        "It all depends on how much information you have about either restaurant,\n",
        "and how many more days you’ll stay in town. If this is your last day, then it’s\n",
        "probably a better idea to go to **Les Epinards**, unless you are expecting **King’s\n",
        "Arm** to be significantly better. However, if you are going to stay there longer,\n",
        "trying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\n",
        "better food for the remaining time, while otherwise you will have missed only\n",
        "one good meal out of many, making the potential risk quite small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC-SVg0ZH7e7",
        "cell_id": "00003-09084f93-5f1a-47ae-95a4-be9916023deb",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Overview\n",
        "* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n",
        "\n",
        "\n",
        "* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zZ6HjxpH7e7",
        "cell_id": "00004-fcb7af58-3083-4302-966a-2d133ea861bb",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Markov Decision Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KurOZxYjH7e7",
        "cell_id": "00005-d667c96a-98aa-4f23-a615-c145933126fb",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "Markov Decision Process (MDP) provides a mathematical framework for modeling sequential decision making under uncertainty. A MDP consists of five parts: the specific decision times, the state space of the environment/system, the available actions for the decision maker, the rewards, and the transition probabilities between the states.\n",
        "\n",
        "* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n",
        "* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n",
        "* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n",
        "* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ for the current state and action, and the resulting next state\n",
        "* Transition probabilities $p(s'|s,a)$ that taking action $a$ in state $s$ will lead to state $s'$\n",
        "\n",
        "At a given decision epoch $t$ and system state $s_t$, the decions maker, or *agent*, chooses an action $a_t$, the system jumps to a new state $s_{t+1}$ according to the transition probability $p(s_{t+1}|s_t,a_t)$, and the agent receives a reward $r_t(s_t,a_t,s_{t+1})$. This process is then repeated for a finite or infinite number of times.\n",
        "\n",
        "A *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state, \n",
        "\n",
        "$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n",
        "\n",
        "where $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if you think all future rewards should count equally, you would use $\\gamma = 1$, while if you only care less about future rewards you would use $\\gamma < 1$. The expected total *discounted* reward becomes\n",
        "\n",
        "$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n",
        "\n",
        "Now, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^{\\pi^*}(s)$ for all $s\\in S$. That is\n",
        "\n",
        "$$V^{\\pi^*}(s) \\geq V^\\pi(s), s\\in S$$\n",
        "\n",
        "The problem of finding the optimal policy is a _dynamic programming problem_. It turns out that a solution to the optimal policy problem in this context is the *Bellman equation*. The Bellman equation is given by\n",
        "\n",
        "$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n",
        "\n",
        "Thus, it can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n",
        "\n",
        "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n",
        "\n",
        "A major drawback of MDPs is called the \"Curse of Dimensionality\". MDPs unfortunately do not scale very well with increasing sets of states or actions.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iUmTgzwH7e7",
        "cell_id": "00006-3afd9287-6032-4fe1-99e0-cdf3dd6fc303",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWPya78-H7e7",
        "cell_id": "00007-f1fd5a6f-a538-4ba7-a5d8-a8871d7e5ceb",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "In this first question we work with the deterministic MDP, no code is necessary in this part.\n",
        "\n",
        "Setup:\n",
        "\n",
        "* The agent starts in state **S**\n",
        "* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n",
        "* Note, that you cannot move outside the grid, thus all actions are not available in every box.\n",
        "* When reaching **F**, the game ends (absorbing state).\n",
        "* The numbers in the boxes represent the rewards you receive when moving into that box. \n",
        "* Assume no discount in this model: $\\gamma = 1$\n",
        "\n",
        "The reward of a state $r(s=(x, y))$ is given by the values on the grid:\n",
        "    \n",
        "| | | |\n",
        "|----------|----------|---------|\n",
        "|-1 |1|**F**|\n",
        "|0|-1|1|  \n",
        "|-1 |0|-1|  \n",
        "|**S**|-1|1|\n",
        "\n",
        "Let $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n",
        "\n",
        "**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle.\n",
        "\n",
        "**Answer**: EENNN, which is not a unique optimum. We could go between points with -1 and +1 reward many times and still get the same total reward. Another path is EENNWNE (same total reward, longer path).\n",
        "\n",
        "**1b)** What is the optimal policy (i.e. the optimal action in each state)?\n",
        "\n",
        "**Answer**: For each state there are several optimal actions, giving same reward. For example in the starting point, both East and North would be optimal action. The table below shows one optimal action for each state.\n",
        "\n",
        "| | | |\n",
        "|----------|----------|---------|\n",
        "|E |E|**F**|\n",
        "|N|N|N|  \n",
        "|N |N|N|  \n",
        "|**E**|E|N|\n",
        "\n",
        "**1c)** What is expected total reward for the policy in 1b)?\n",
        "\n",
        "**Answer**: \n",
        "\n",
        "Following the policy in the table in 1b and summing up, the total reward comes out to be -1+1-1+1+0 = 0 from EENNN.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyQ7IatcH7e7",
        "cell_id": "00008-81ec2497-b05a-4296-bd51-56df3cb6de19",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NfqElM_H7e7",
        "cell_id": "00009-cc0c8d06-c997-4b31-bdea-7b6cee817b88",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation. The process of repeated application of the Bellman equation what we here call the _value iteration_ algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qbn4HjqR2fA",
        "cell_id": "00010-941fad6f-13a1-4e47-bc23-d1f74ecbb754",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "The value iteration algorithm practically procedes as follows:\n",
        "\n",
        "```\n",
        "epsilon is a small value, threshold\n",
        "for x from i to infinity \n",
        "do\n",
        "    for each state s\n",
        "    do\n",
        "        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
        "    end\n",
        "    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n",
        "        for each state s,\n",
        "        do\n",
        "            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
        "            return π, V_k \n",
        "        end\n",
        "end\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7hOzat7H7e8",
        "cell_id": "00011-5d31e1d4-7062-4958-ac26-5c555216da8d",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability of 0.8 that that action will be performed and a probability of 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state, that is, we stay on the grid), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|0|0|\n",
        "|0|10|0|  \n",
        "|0|0|0|  \n",
        "\n",
        "\n",
        "**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|8|0|\n",
        "|8|2|8|  \n",
        "|0|8|0|  \n",
        "  \n",
        "**Iteration 2**:  \n",
        "  \n",
        "Staring with cell (0,0) (lower left corner): We find the expected value of each move:  \n",
        "Action **S**: 0  \n",
        "Action **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action **W**: 0\n",
        "\n",
        "Hence any action between **E** and **N** would be best at this stage.\n",
        "\n",
        "Similarly for cell (1,0):\n",
        "\n",
        "Action **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n",
        "\n",
        "Similar calculations for remaining cells give us:\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|5.76|10.88|5.76|\n",
        "|10.88|8.12|10.88|  \n",
        "|5.76|10.88|5.76|  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccoMLc71H7e8",
        "cell_id": "00012-618f9160-62f0-4494-aa00-3fc82d613ed1",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "**2a)** Implement the value iteration algorithm just described here in python, and show the converging optimal value function and the optimal policy for the above 3x3 grid. Hint: use the pseudo-code above as a starting point, but be sure to explain what every line does.\n",
        "\n",
        "**Answer:** Code (see below). Optimal value function and optimal policy (see output from code)\n",
        "\n",
        "**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$.\n",
        "\n",
        "**Answer:** Bellman equation convergence is determined by the discount factor gamma. The reason the result does not depend on the initial V values is \n",
        "that gamma < 1.0. Since the discount factor is < 1 it will, \n",
        "after many iterations, be of such a high power that it makes the initial V value \"disappear\". If gamma was equal to 1.0, the initial value would have an impact on the end result. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00013-da30323b-685a-4614-b359-b6108c8c6b94",
        "output_cleared": false,
        "deepnote_cell_type": "markdown",
        "id": "RyYHPRYQQs5h"
      },
      "source": [
        "## 2a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-9c4b7c48-7ae4-4dbf-8890-c46fe3215b5e",
        "output_cleared": false,
        "source_hash": "bda9b6c",
        "execution_millis": 49,
        "execution_start": 1607349459599,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKtXMkikQs5i",
        "outputId": "3c8afd83-574f-46ea-8865-3b180a2c145e"
      },
      "source": [
        "# We used the code from the provided notebook \"q_learninig_froze_lake\" as a starting point \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "rewards = np.array([[0,0,0],\n",
        "                    [0,10,0],\n",
        "                    [0,0,0]])\n",
        "\n",
        "\n",
        "p_move = 0.8\n",
        "p_notmove = 1- 0.8\n",
        "df = 0.9\n",
        "epsilon = 0.001 #Convergence criteria\n",
        "\n",
        "\n",
        "# Get all possible actions and the resulting states\n",
        "# and return as 2 arrays\n",
        "def possible_actions(g, s) :\n",
        "    pa = np.zeros((4), dtype = bool)\n",
        "    sp = np.zeros((4, 2), dtype = int)\n",
        "    # East\n",
        "    if s[1]+1 <= g.shape[1]-1 :\n",
        "        pa[0] = True\n",
        "        sp[0] = [s[0], s[1]+1] \n",
        "    # West\n",
        "    if s[1]-1 >= 0 :\n",
        "        pa[1] = True \n",
        "        sp[1] = [s[0], s[1]-1] \n",
        "    # North\n",
        "    if s[0]-1 >= 0 :\n",
        "        pa[2] = True \n",
        "        sp[2] = [s[1], s[0]-1] \n",
        "    # South\n",
        "    if s[0]+1 <= g.shape[0]-1 :\n",
        "        pa[3] = True\n",
        "        sp[3] = [s[1], s[0]+1] \n",
        "    return pa, sp \n",
        "\n",
        "# Get the reward for moving to a state\n",
        "def get_reward(r, s) :\n",
        "    return r[s[0]] [s[1]] \n",
        "\n",
        "# Get the statevalue for a state\n",
        "def get_V(g, s) :\n",
        "    return g[s[0]] [s[1]] \n",
        "\n",
        "\n",
        "# Calculate a new statevalue\n",
        "def maxaction(g, r, s) :\n",
        "    v = 0\n",
        "    mv = 0 \n",
        "    pa, sp = possible_actions(g, s)\n",
        "    # For all possible actions\n",
        "    for i in range (4) :\n",
        "        # Chose the one with the highest value and return it\n",
        "        if pa[i] :\n",
        "            v = p_move * (get_reward(r, sp[i]) + df * get_V(g, sp[i])) + p_notmove * (get_reward(r, s) + df * get_V(g, s))\n",
        "            if v > mv :\n",
        "                mv = v\n",
        "    return mv\n",
        "\n",
        "\n",
        "# Get the policy matrix from the value function\n",
        "def get_policy (V) :\n",
        "    policy = np.zeros((3, 3), dtype = int)\n",
        "    # For every state\n",
        "    for i in range (V.shape[0]) :\n",
        "        for j in range  (V.shape[1]) :\n",
        "            tmp = 0\n",
        "            # Get all possible and their corresponding state \n",
        "            pa, sp = possible_actions(V, [i, j])\n",
        "            for a in range (4) :\n",
        "                if pa[a] :                   \n",
        "                    # Chose the action that gives the highest value and insert that value in\n",
        "                    # the decision policy matrix. If there are 2 decisions thar are equal we \n",
        "                    # will return the first found    \n",
        "                    if get_V(V, sp[a]) > tmp :\n",
        "                        policy[i][j] = a\n",
        "                        tmp = get_V(V, sp[a])\n",
        "    return policy\n",
        "\n",
        "\n",
        "# Translate the numbers in the policy matrix the readable directions\n",
        "def translate_policy (p) :\n",
        "    translated_policy = np.zeros((3,3), dtype=str)\n",
        "    for i in range (V.shape[0]) :\n",
        "        for j in range  (V.shape[1]) :\n",
        "            if p[i][j] == 0 :\n",
        "                translated_policy[i][j]  = 'East'\n",
        "            elif p[i][j] == 1 :\n",
        "                translated_policy[i][j]  = 'West'\n",
        "            elif p[i][j] == 2 :\n",
        "                translated_policy[i][j]  = 'North'\n",
        "            elif p[i][j] == 3 :\n",
        "                translated_policy[i][j]  = 'South'\n",
        "    return translated_policy\n",
        "\n",
        "            \n",
        "# Main program, makes the value iteration to create a value function and prints it out\n",
        "# together with a decision policy \n",
        "V_new = np.zeros((3,3))\n",
        "while (True) :\n",
        "    converged = True\n",
        "    V = np.copy(V_new)\n",
        "    # For every state\n",
        "    for i in range (V.shape[0]) :\n",
        "        for j in range  (V.shape[1]) :\n",
        "            # Get the new highest value and update the new value function\n",
        "            V_new[i][j] = maxaction(V, rewards, [i,j])\n",
        "            # Check for convergence\n",
        "            if  abs(V_new[i][j] - V[i][j]) > epsilon :\n",
        "                converged = False\n",
        "    if converged : \n",
        "        break \n",
        "\n",
        "\n",
        "# Print the results\n",
        "print('****Optimal policy****')\n",
        "print(translate_policy(get_policy(V_new)))\n",
        "print()\n",
        "print('****Optimal Value function****')\n",
        "print(np.around(V_new,2))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****Optimal policy****\n",
            "[['E' 'S' 'W']\n",
            " ['E' 'E' 'W']\n",
            " ['E' 'N' 'W']]\n",
            "\n",
            "****Optimal Value function****\n",
            "[[45.6  51.94 45.6 ]\n",
            " [51.94 48.04 51.94]\n",
            " [45.6  51.94 45.6 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00015-6d6078c8-de47-48bf-bae9-d9b565ae0321",
        "output_cleared": false,
        "deepnote_cell_type": "markdown",
        "id": "KxQvUN0bQs5j"
      },
      "source": [
        "**DISCUSSION:** Please note that the decison policy is not the only optimal. E.g. if we are in the left bottom corner, North is an as valid \n",
        "action as East. The same goes for all the corner states but if you are next to the state in the middle, the optimal action is \n",
        "always to \"go\" to the center. I this case we chosed to make the decision policy this way but one could as well have it non deterministic \n",
        "since more than one action is valid. What to choose depends on the application.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQXoOa7LH7e8",
        "cell_id": "00013-e4500da0-540f-47b5-bc0a-2960e05a6d50",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Reinforcement Learning (RL)\n",
        "Until now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(a,s,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n",
        "\n",
        "So far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n",
        "\n",
        "$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s,s') + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "The value function and the action-value function are directly related through\n",
        "\n",
        "$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n",
        "\n",
        "i.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n",
        "\n",
        "$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s\n",
        "]\\,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "and the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n",
        "\n",
        "$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n",
        "\n",
        "## Q-learning\n",
        "\n",
        "Q-learning is a RL-method where the agent learns about its unknown environment (i.e. the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n",
        "\n",
        "$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \n",
        "\n",
        "where $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there is sufficient exploration. While a constant $\\alpha$ generally does not guarantee us to reach true convergence, we keep it constant at $\\alpha=0.1$ for this assignment.\n",
        "\n",
        "## OpenAI Gym\n",
        "\n",
        "We shall use already available simulators for different environments (worlds) using the popular OpenAI Gym library. It just implements [different types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as the [Chain enviroment](https://gym.openai.com/envs/NChain-v0/) illustrated below.\n",
        "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\n",
        "The figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n",
        "\n",
        "## Question 3\n",
        "You are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the Q-learning algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Convergence is **not** a constant value, rather a stable plateau with some noise. Take $\\gamma=0.95$. You can refer to the Q-learning (frozen lake) Jupyter notebook shown in class, uploaded on Canvas. Hint: start with a small learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00017-03a550ad-9d50-46de-b09d-acdd44f1bb7c",
        "output_cleared": false,
        "source_hash": "e1a35488",
        "execution_millis": 61556,
        "execution_start": 1607508844707,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "lrF8Ux0FQs5k",
        "outputId": "eb057f8f-17ee-4e02-f739-56f2d57b134a"
      },
      "source": [
        "# Based on the notebook \"q_learning_frozen_lake\"\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Leraning rate, high learning rate means updating Q in big steps\n",
        "lr = 0.0005\n",
        "\n",
        "# Discount factor\n",
        "df = 0.95\n",
        "\n",
        "# Epsilon, if e=1 --> always Explore, if e=0 --> Always exploit\n",
        "e = 0.1 #Explore with prob=0.1\n",
        "\n",
        "# Number of episodes we will use for training\n",
        "episodes = 5000\n",
        "\n",
        "# create the environment\n",
        "env = gym.make('NChain-v0') \n",
        "\n",
        "# List used to check for conversion when finished\n",
        "conv_list = []\n",
        "\n",
        "# Create our Q matrix\n",
        "Q = np.zeros([5, 2])\n",
        "\n",
        "\n",
        "\n",
        "for _ in range (episodes):\n",
        "    # reset the environment\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    # Make a copy of the Q matrix\n",
        "    Q_old = np.copy(Q)\n",
        "    # Repeat untiol the environment tells us we are done\n",
        "    while done == False :\n",
        "        # Chose a action\n",
        "        if random.uniform(0, 1) < e :\n",
        "            action = env.action_space.sample()\n",
        "        else :\n",
        "            action = np.argmax(Q[state,:])\n",
        "        # Then we perform the action and receive the feedback from the environment\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        update = reward + (df*np.max(Q[new_state,:])) - Q[state, action]\n",
        "        Q[state,action] += lr*update \n",
        "        state = new_state\n",
        "    # Add the sum of the elemnts to the list in order to plot it when we are done \n",
        "    conv_list.append(np.sum(Q))\n",
        "\n",
        "plt.title('Convergence of the Q-table')\n",
        "plt.xlabel('Number of episodes')\n",
        "plt.ylabel('Sum of all elements in the Q-matrix')\n",
        "x = np.array([i for i in range(len(conv_list))]) \n",
        "y = conv_list\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.show()\n",
        "\n",
        "print('*** Q-matrix ***')\n",
        "print(np.around(Q,2))\n",
        "\n",
        "print('Column 1 represents action a and column 2 represents action b')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "*** Q-matrix ***\n",
            "[[61.37 60.68]\n",
            " [64.92 61.62]\n",
            " [69.56 62.77]\n",
            " [75.41 64.19]\n",
            " [83.07 66.25]]\n",
            "Column 1 represents action a and column 2 represents action b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00019-001ce07a-66da-4b8e-a000-6cc6b7c19037",
        "output_cleared": false,
        "deepnote_cell_type": "markdown",
        "id": "TvoL41WaQs5k"
      },
      "source": [
        "**DISCUSSION Q3:** In the results above we can see the that the Q-matrix converges. The measurement we selected for convergence was the sum of \n",
        "all the elements in the Q-matrix. We think that was a simple solutions that  works fine in this case since we know the \n",
        "algorithm will stabilize around certain values with som noice and therefore it will stabilize around a certain sum.\n",
        "\n",
        "We picked the learning rate using trail and error. As always picking the leraning rate is trade between speed of convergence,\n",
        "not getting stuck in local minimas, and not being too noisy around the global minima (optimal solution) we are looking for. \n",
        "In our case a learning rate of 0.0005 worked fine. It converges pretty fast and it ends up in stable state.\n",
        "\n",
        "We can also conclude that the Q-matrix gives us the correct action in every state. Using it we will always take action \"a\" which \n",
        "is the correct action given this environment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY0iEZuKd4RO"
      },
      "source": [
        "\r\n",
        "## Question 4\r\n",
        "\r\n",
        "**4a)** Define the MDP corresponding to the Chain environment above and verify that the optimal $Q^*$ value obtained using simple Q-learning is the same as the optimal value function $V^*$ for the corresponding MDP's optimal action. Hint: compare values obtained using value iteration and Q-learning.\r\n",
        "\r\n",
        "**4b)** What is the importance of exploration in RL? Explain with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00019-79696b0e-15e6-4487-abd9-c63ce944c6d9",
        "output_cleared": false,
        "deepnote_cell_type": "markdown",
        "id": "DAJUBp-KQs5k"
      },
      "source": [
        "## 4a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00020-e633d846-d526-4e36-a6f8-7a867ced5697",
        "output_cleared": false,
        "source_hash": "d7419347",
        "execution_millis": 2,
        "execution_start": 1607509512755,
        "deepnote_cell_type": "code",
        "id": "8G7J8csNQs5k",
        "outputId": "e43607c5-40fc-450b-dc49-650d7de0a27b"
      },
      "source": [
        "# Representation of the \"NChain-v0\"-environment\n",
        "# Each row is a state\n",
        "# The first tuple is the reward for action a and b respectively\n",
        "# The second tuple is the the next state for action a and b respectively\n",
        "reward_state = np.array([[(0, 2), (1, 0)],\n",
        "                        [(0, 2), (2, 0)],\n",
        "                        [(0, 2), (3, 0)],\n",
        "                        [(0, 2), (4, 0)],\n",
        "                        [(10, 2), (4, 0)]])\n",
        "\n",
        "# Probability to make the move choosen, taken from the documentation for the NChain-v0\"-environment\n",
        "prob_move = 0.8\n",
        "prob_slip = 1- prob_move\n",
        "\n",
        "# Number used to measure convergence\n",
        "eps = 0.01\n",
        "\n",
        "# Initiate the value function\n",
        "V_new = np.zeros((5))\n",
        "\n",
        "# Main program, calculates the Value function\n",
        "while (True) :\n",
        "    converged = True\n",
        "    V = np.copy(V_new)\n",
        "    # For everey state\n",
        "    for i in range (V.shape[0]) :\n",
        "        # Calculate the value for both action a and b\n",
        "        aa = prob_move * (reward_state[i][0][0] + df*V[reward_state[i][1][0]]) + prob_slip * (reward_state[i][0][1] + df*V[reward_state[i][1][1]])\n",
        "        ab = prob_move * (reward_state[i][0][1] + df*V[reward_state[i][1][1]]) + prob_slip * (reward_state[i][0][0] + df*V[reward_state[i][1][0]])\n",
        "        # Chose the largest value\n",
        "        if aa >= ab :\n",
        "            V_new[i] = aa\n",
        "        else :\n",
        "            V_new[i] = ab\n",
        "        # Check for convergence\n",
        "        if  abs(V_new[i] - V[i]) > eps :\n",
        "            converged = False\n",
        "    if converged : \n",
        "        break \n",
        "print('****Value function****')\n",
        "print(np.around(V_new, 2))\n",
        "print()\n",
        "print('*** Q* value from part 3 ***')\n",
        "print(np.around(Q[:,0],2))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****Value function****\n",
            "[61.19 64.71 69.33 75.41 83.41]\n",
            "\n",
            "*** Q* value from part 3 ***\n",
            "[61.64 65.17 69.82 75.78 83.37]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00022-9b1e74ce-dcfc-418c-a494-cd912c1d1dfb",
        "output_cleared": false,
        "deepnote_cell_type": "markdown",
        "id": "Q8rIZHAKQs5l"
      },
      "source": [
        "**DISCUSSION Q4a:** Here we can see that the Value function we get, corresponds very well to optimal actions in the Q-matrix in part 3. This tells \n",
        "us thet the Q-learning algortihm found an optimal soultion in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00017-c210b970-3543-4aa9-a2db-746a6feaa557",
        "output_cleared": false,
        "deepnote_cell_type": "markdown",
        "id": "SEYQC9IKQs5l"
      },
      "source": [
        "**Answer 4b:** The RL algorithm does not know the optimal solution beforehand. The aim is to find it by exploration and exploitation. Iterating through different scenarios, \n",
        "options with different total rewards are found. Having a _greedy_ approach, i.e. high level of exploitation where the most rewarding action at the present \n",
        "moment are taken, large part of the state space could be lost risking ending up in a local optimum. However, with a non-greedy (or _Epsilon greedy_) approach there is high level \n",
        "of exploration which lead to low level of learing from experience and hence, small improvements. This means that there is a trade-off between exploration and \n",
        "exploitation. The _decaying Epsilon_ method tries to decrease the percentage dedicated for exploration as time goes by since the value of the \n",
        "gained infomation from exploration will decrease as time goes by. This gives a good balance between exploration and exploitation. \n",
        "\n",
        "\n",
        "We are all confronted with these kind of descisions on daily basis: Should I keep my job or find a new one? Should I buy a new kind of yoghurt or buy the same as before?\n",
        "To illustrate this with an exmaple, we consider the problem described in the introduction of this report: \n",
        "choosing restaurants on vacation where the quality of the restaurants are not known beforehand. Maybe you find \n",
        "a _good_ restaurant you like on the first night and then you go to this restaurant each night (no exploration). Then you might miss out on other, better options. \n",
        "This would be the greedy approach, meaning you are maximizing the expected \n",
        "reward each day seperately while not thinking about future potential reward. On the other hand, if you are trying to find a new, better restaurant each day, \n",
        "you will not utilize the learning/experiences from previous nights and the expected reward would be corrosponding to an average restaurant \n",
        "(of course assuming not reading reviews). The would be the Epsilon greedy approach.\n",
        "A reasonable trade off would in this case would be to have high level of exploration for a couple days and then go to the restaurant you liked best the \n",
        "rest of the time. This would be the decaying Epsilon approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWUqaN60H7e8",
        "cell_id": "00014-1d073534-e9ad-443b-b67e-ab0b3de65157",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "**5a)** Give a summary of how a decision tree works and how it extends to random forests.\n",
        "\n",
        "**Answer:** In general a decision tree ask questions and based on the answers of the questions, it classifies the object. The answers to the questions is categorial/discrete (often 2 options but could be more). The question could be based on numerics (for example: Are you taller than 1.8 m? YES/NO). Then you arrange the questions in a tree structure with the _Root node_ at the top representing the first question asked. Then the root node branches (each branch represent an answer to the question) into 2 new nodes which could be either _internal node(s)_ (which would be a new question) and/or a _leaf node(s)_ (bottom of the tree). For a new _internal node_ there is a new question that branches to a leaf and/or internal nodes. Then you follow the tree from top to bottom until you reach a leaf node. The leaf node represent the outcome of the object subjected to classification.\n",
        "\n",
        "When building a decision tree, the question containing most information (i.e. classifies the data set with highest accuaracy) are chosen as root node. And then you choose the questions in descending order (w.r.t information). This is often measured by Gini Impurity which is the probability of assigning an object to the wrong class for a given question. Hence the lowest Gini impurity will be at the top of the tree.\n",
        "\n",
        "A random forest is using a form of decision tree, but instead of each question/node being selected only a subset of the nodes are taken into consideration as candidates for each nextcoming node. Another difference is that it is built on randomly selected data points rather than the entire set of data. The random forest is then built up by a collection of different sub decision trees, and uses the average of the trees to decide on the predicted class for a observation.\n",
        "\n",
        "\n",
        "The example below shows a descision tree to predict wheather a person has heart disease:\n",
        "\n",
        "| | | | |  \n",
        "|----------|----------|---------|---------|\n",
        "|Age over 40?|Eats junkfood 3+ days a week|Exercises 2+ times a week|**Has heartdisease?**|\n",
        "|Yes|Yes|No|**Yes**|\n",
        "|No|Yes|No|**Yes**|\n",
        "|Yes|No|Yes|**No**|\n",
        "|No|No|Yes|**No**|\n",
        "\n",
        "Where the last column represents the target.\n",
        "\n",
        "In a decision tree, all 3 parameters are considered when deciding what node should be the root node. For an arbitrary tree in the random forest, we might only include the first two parameters as the root node, pick the second one and so on. \n",
        "\n",
        "For a regular decision tree, the feature of the root node will have a higher impact on the prediction outcome than nodes further down the tree. With a random forest consisting of multiple trees with subsets of the parameters, the node order will be different for different sub-trees, making the prediction less sensitive. A decision tree is also more sensitive to overfitting, as the accuracy increase when you include more nodes. \n",
        "\n",
        "\n",
        "\n",
        "**5b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification.\n",
        "\n",
        "**Answer:** In supervised learning the algorithm learns from labeled data. Meaning that every input comes with a target. The “job” of \n",
        "the algorithm is to make a function mapping of the input data and the targets. The goal is that the mapping will \n",
        "generalize to new unseen input data.  \n",
        "\n",
        "Reinforcement learning on the other hand does not require any data. Instead we have an agent which acts upon an environment \n",
        "and get rewards depending on its action. One can say that the agent creates its own training data by interacting with the \n",
        "environment. The algorithm learns by trail and error. It samples from possible actions, observes the outcome and , hopefully, \n",
        "it learns an optimal strategy for the problem at hand.\n",
        "\n",
        "The differences makes them suitable for different purposes. RL is often used in AI applications due to its interactive nature while supervised learning are used to gain insights from large amount of data and predicting future outcome based on historical data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wev-_UhcH7e8",
        "cell_id": "00015-6aebcb64-ea53-4452-a396-f2432f1f0ca0",
        "output_cleared": false,
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "\n",
        "# References\n",
        "Primer/text based on the following references:\n",
        "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
        "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"
      ]
    }
  ]
}